---

########     Use mysql_db ansible plugin to dump databases from dev and push to downstream environments:  qa and/or prod        ##########

- name: Create Backups folder on dev
  file: path={{ mysql_datadump_src_dir }} state=directory
  run_once: true
  delegate_to: dev.opentlc.com

- name: Dump remote dev dashbuilder database
  mysql_db: 
    login_user: root
    login_host: localhost
    login_password: "{{ db_root_passwd }}"
    name: "{{ item }}"
    state: dump 
    target: "{{ mysql_datadump_src_dir }}/{{ item }}Db.sql.gz"
  with_items: "{{databases_to_update_from_dev}}"
  delegate_to: dev.opentlc.com

- name: Create following directory from which to put fetched database zips; local:{{mysql_datadump_target_dir}}
  command: mkdir -p {{ mysql_datadump_target_dir }}
  delegate_to: localhost

- name: Copy database dump files from dev to local:{{mysql_datadump_target_dir}}
  fetch:
    src: "{{ mysql_datadump_src_dir }}/{{ item }}Db.sql.gz"
    dest: "{{ mysql_datadump_target_dir }}"
    flat: yes
  with_items: "{{databases_to_update_from_dev}}"
  delegate_to: dev.opentlc.com

- name: Create Backups folder on remote {{gpte_env}} environment
  file: path={{ mysql_datadump_target_dir }} state=directory

- name: push database dump files from local to remote {{gpte_env}} environment
  copy:
    src: "{{ mysql_datadump_target_dir }}/{{ item }}Db.sql.gz"
    dest: "{{ mysql_datadump_target_dir }}"
  with_items: "{{databases_to_update_from_dev}}"

- name: import data into {{gpte_env}} RDBMS
  mysql_db:
    name: "{{ item }}"
    state: import 
    target: "{{ mysql_datadump_target_dir }}/{{ item }}Db.sql.gz"
  with_items: "{{databases_to_update_from_dev}}"
 

######      Fine grained database updates from dev --> prod         ##############
# 
#  There are use cases where fine grained data changes are made in the GPTE Reporting development environment.
#  One example is changes to: lms_transactional.CourseMappings .
#  These fine grained data changes are maintained in SQL scripts in the GPTE dev environment. 
#  These SQL scripts need to get executed in in qa and prod

# lms_transactional updates
- name: Determine if {{mysql_update_lms_trans_filename}} file exists in dev environment
  stat:
    path: "{{database_update_script_dir}}/{{mysql_update_lms_trans_filename}}"
  register: update_lms_trans_file
  delegate_to: dev.opentlc.com
- debug:
    msg: "Does the lms_trans update file exist on dev ?  {{update_lms_trans_file.stat.exists}}"

- name: Copy {{mysql_update_lms_trans_filename}} from dev to local:{{mysql_datadump_target_dir}}
  fetch:
    src: "{{database_update_script_dir}}/{{mysql_update_lms_trans_filename}}"
    dest: "{{ mysql_datadump_target_dir }}"
    flat: yes
  delegate_to: dev.opentlc.com
  when: "{{update_lms_trans_file.stat.exists}} == True"

- name: push {{mysql_update_lms_trans_filename}} from local to remote {{gpte_env}} environment
  copy:
    src: "{{ mysql_datadump_target_dir }}/{{mysql_update_lms_trans_filename}}"
    dest: "{{ mysql_datadump_target_dir }}"
  when: "{{update_lms_trans_file.stat.exists}} == True"

- name: Execute commands in {{mysql_update_lms_trans_filename}}
  mysql_db:
    login_user: "{{lms_trans_userId}}"
    login_password: "{{ lms_trans_passwd }}"
    name: "{{lms_trans_db_name}}"
    state: import
    target: "{{ mysql_datadump_target_dir }}/{{mysql_update_lms_trans_filename}}"
  when: "{{update_lms_trans_file.stat.exists}} == True"
